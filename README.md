# OTUS HW

## ДЗ по модулю "Ansible роли, управление настройками нескольких окружений и best practices"

* Перенес созданные впрошлых ДЗ плейбуки в раздельные роли
* Описал два окружения (окружение по-умолчанию - **stage**)
* Подключил коммьюнити роль nginx
* Использовал Ansible Vault для окружений

Динамический inventory генерируется ```Terraform```'ом.

Для сборки:

* перейти в каталог **terraform/stage** или **terraform/prod**, выполнить команды:

    ``` bash
    terraform init
    terraform apply
    ```

* в файле ```environments/{stage,prod}/group_vars/app``` заполнить значение переменной ```db_host```, подставив IP-адрес VM с базой данных (внутренний или внешний)
* в каталоге **ansible** запустить плэйбук командой:

    > Для настройки окружения **stage**

    ``` bash
    ansible-playbook playbooks/site.yml
    ```

    > Для настройки окружения **prod**

    ``` bash
    ansible-playbook -i environments/prod/inventory playbooks/site.yml
    ```

Для проверки:

* ``` bash
  terraform output
  ```

* ```external_ip_address_app``` - адрес VM с приложением из вывода output переменной
* открыть в браузере <http://external_ip_address_app:9292>

---

## ДЗ по модулю "Продолжение знакомства с Ansible: templates, handlers, dynamic inventory, vault, tags"

В процессе выполнения реализовал несколько сценариев:

* Один playbook, один сценарий
* Один плейбук, несколько сценариев
* Несколько плейбуков

Так же создал теги, хэндлеры, шаблоны файлов, реализовал провижининг ```Packer```'а с помощью ```Ansible```.

Для сборки ```Packer```-образов:

* нужно добавить в ```Packer```-шаблоны (```{app,db}.json```), содержащие описание образов VM, в раздел ```provisioners``` строку

    ``` json
    "use_proxy": "false"
    ```

   > Убрал ее из-за того, что тесты не знают такой параметр

* выполнить сборку образов:

    ``` bash
    packer build -var-file=packer/variables.json packer/app.json
    packer build -var-file=packer/variables.json packer/db.json
    ```

Для выполнения данного ДЗ отключил провижининг в ```Terraform```. В окружении **stage** используются образы, созданные ```Packer```'ом с провижинингом ```Ansible```. Динамический inventory генерируется ```Terraform```'ом.

Для сборки:

* перейти в каталог **stage** или **prod**, выполнить команды:

    ``` bash
    terraform init
    terraform apply
    ```

* в плэйбуке ```ansible/app.yml``` заполнить значение переменной ```db_host```, подставив IP-адрес VM с базой данных (внутренний или внешний)
* запустить плэйбук командой:

    ``` bash
    ansible-playbook site.yml
    ```

Для проверки:

* ``` bash
  terraform output
  ```

* ```external_ip_address_app``` - адрес VM с приложением из вывода output переменной
* открыть в браузере <http://external_ip_address_app:9292>

---

## ДЗ по модулю "Принципы организации инфраструктурного кода и работа над инфраструктурой в команде на примере Terraform"

* Создание ресурсов разнесено по модулям:
  * виртуальная сеть
  * база данных
  * приложение
* Создал окружения **stage** и **prod** на основе модулей
* Настроил aws cli для работы со стораджем
* Настроил хранение стейт файла в удаленном бекенде
* Добавил необходимые provisioner в модули для деплоя и работы приложения, включая проброс IP адреса базы

Для сборки перейти в каталог **stage** или **prod**, выполнить команды:

``` bash
terraform init
terraform apply
```

Для проверки:

* ``` bash
  terraform output
  ```

* ```external_ip_address_app``` - адрес HTTP балансировщика из вывода output переменной
* открыть в браузере <http://external_ip_address_app:9292>

---
## ДЗ по модулю "Знакомство с Terraform"

* Создал сервисный аккаунт, делегировал ему минимально необходимые права (все команды обернул в скрипт).
* Описал ресурс для создания инстанса VM в YC из base-образа, созданного в предыдущем ДЗ.
* Реализовал вывод переменных (IP адрес инстанса) в консоль (```outputs.tf```).
* Добавил провижинеры, скрипт, unit-файл для деплоя последней версии приложения на созданную VM.
* Создал конфигурационные файлы ```variables.tf```, ```terraform.tfvars```, ```terraform.tfvars.example```, в которых определил входные переменные
* Описал ресурс для создания HTTP балансировщика, направляющего трафик на наше развернутое приложение на инстансе reddit-app (файл ```lb.tf```). Добавил в output переменные адрес балансировщика.
* Добавил в код еще один terraform ресурс для нового инстанса приложения reddit-app2, добавил его в балансировщик, при остановке на одном из инстансов приложения (```systemctl stop puma```),приложение продолжает быть доступным по адресу балансировщика. Добавил в output переменные адрес второго инстанса.

  Основные проблемы данной конфигурации:
  * дублирование кода,
  * необходимость правки частей кода, связанных с именами/адресами ресурсов/инстансов, что требует особой внимательности)))

* Реализовал подход с заданием количества инстансов через параметр ресурса count. Переменная count задается в параметрах и по умолчанию равна 1.

Для сборки выполнить команды:

``` bash
terraform init
terraform apply
```

Для проверки:

* ``` bash
  terraform output
  ```

* ```external_ip_address_lb``` - адрес HTTP балансировщика из вывода output переменной
* открыть в браузере <http://external_ip_address_lb>

---
### ДЗ по модулю PACKER


```
testapp_IP = 130.193.38.240
testapp_port = 9292
```

Создать ветку packer-base

Перенести файлы предыдущего ДЗ в configscripts

Установил Packer на локальную машину

Создал сервисный аккаунт, делегировал права аккаунту для Packer

Создал шаблон описания образа VM с предустановленным Ruby и MongoDB

Сбилдил образ с приложением внутри

Параметризировал шаблон

Выполнил все доп.задания

### ДЗ по модулю Основные сервисы Yandex Cloud


```
testapp_IP = 84.252.128.120
testapp_port = 9292
```

Установил и настроил yc CLI для работы с аккаунтом,
создадал хост с помощью CLI,
установил на нем ruby для работы приложения,
установил и запустил MongoDB,
задеплоил тестовое приложение, запустил и проверил его работу.
Все необходимые команды завернул в скрипты.
Создал скрипт для создания инстанса с уже запущенным приложением.

Два варианта запуска инстанса:

Первый:

    ```
    yc compute instance create \
        --name reddit-app \
        --hostname reddit-app \
        --memory=4 \
        --create-boot-disk image-folder-id=standard-images,image-family=ubuntu-1604-lts,size=10GB \
        --network-interface subnet-name=default-ru-central1-a,nat-ip-version=ipv4 \
        --metadata serial-port-enable=1 \
        --ssh-key ~/.ssh/appuser.pub
    ```
    Далее Скопировать на созданную ВМ файлы:

    - ```install_ruby.sh```
    - ```install_mongodb.sh```
    - ```deploy.sh```
    - ```startup.sh```

    Запустить скрипт ```startup.sh```

Второй:

Запустить скрипт ```yc_instance_init.sh```

---
### ДЗ по модулю Знакомство с облачной инфраструктурой. Yandex.Cloud
bastion_IP = 158.160.106.34

someinternalhost_IP = 10.128.0.18

vpn-host https://158.160.106.34.sslip.io

Для подключения к хосту без внешнего IP в одну команду необходимо использовать bastion как jump хост:
```
	ssh -J appuser@bastion appuser@someinternalhost
  ```

Для подключения вида ```ssh someinternalhost``` необходимо внести следующие правки в файл (создать alias) ``` ~/.ssh/config:```
```
### Jump host
Host bastion
  HostName 158.160.106.34
  user appuser

##### Destination host
Host someinternalhost
  HostName 10.128.0.18
  ProxyJump bastion
  user appuser
```
При этом bastion так же используется как jump хост, но так же можно настроить на bastion-хосте VPN.
